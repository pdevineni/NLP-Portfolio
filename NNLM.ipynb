{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnZvg/ImfGRytk7if70JZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdevineni/NLP-Portfolio/blob/main/NNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Neural Probabilistic Language Model**\n",
        "\n",
        "This work is based on the 2003 paper by Yoshua Bengio et al. titled **A Neural Probabilistic Language Model**\n",
        "\n",
        "**Task:** Develop a neural probabilistic language model that can generalize to unseen data.\n",
        "\n",
        "**Literature Survey:** Previous statistical language models have been limited by their inability to generalize to unseen data. This is because they are based on n-gram models, which only consider the last n-1 words in a sequence when predicting the next word. This means that they cannot handle new combinations of words that were not seen in the training data.\n",
        "\n",
        "**Approach:** The proposed approach is to use a neural network to learn a probability distribution over the next word, given all the previous words in a sequence. The neural network is trained on a large corpus of text, and it is able to learn the statistical relationships between words. This allows the model to generalize to unseen data, even if it contains new combinations of words.\n",
        "\n",
        "**Architecture:** \n",
        "The neural network architecture used in this paper is a recurrent neural network. Recurrent neural networks are able to learn long-range dependencies by feeding the output of the network back into the input. This allows the network to learn the probability distribution of words in a language, even if the words are separated by a long distance.\n",
        "\n",
        "- The input layer consists of a vector of word embeddings, where each word embedding represents the meaning of the word.\n",
        "- The hidden layer consists of a number of neurons, which are connected to the input layer by a weight matrix.\n",
        "- The output layer consists of a vector of word probabilities, where each word probability represents the probability of the next word being that word.\n",
        "\n",
        "**Novelty:** The novelty of this approach is that it is able to learn a probability distribution over the next word, given all the previous words, without explicitly stating the statistical relationships between words in the training corpus. This allows the model to generalize to new combinations of words that were not seen in the training corpus.\n",
        "\n",
        "**Results:** The neural probabilistic language model is able to generalize better than previous statistical language models.\n",
        "The neural probabilistic language model is able to achieve state-of-the-art results on a number of language modeling benchmarks.\n",
        "\n",
        "This paper has made significant contributions to the field of language modeling. Neural probabilistic language models have become the state-of-the-art approach to language modeling, and they are used in a variety of applications, such as speech recognition, machine translation, and text generation.\n"
      ],
      "metadata": {
        "id": "b9pu6-w9DMuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gWGDcQ7oDGGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "XfbaGQ5mDRIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch():\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for sen in sentences:\n",
        "        word = sen.split() # space tokenizer\n",
        "        input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input\n",
        "        target = word_dict[word[-1]] # create (n) as target, We usually call this 'casual language model'\n",
        "\n",
        "        input_batch.append(input)\n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, target_batch"
      ],
      "metadata": {
        "id": "b5wZzh9yH85u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "shrNZuBEJLdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class NNLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNLM, self).__init__()\n",
        "        # The embedding layer in Py\n",
        "        self.C = nn.Embedding(n_class, m)\n",
        "        self.H = nn.Linear(n_step * m, n_hidden, bias=False)\n",
        "        self.d = nn.Parameter(torch.ones(n_hidden))\n",
        "        self.U = nn.Linear(n_hidden, n_class, bias=False)\n",
        "        self.W = nn.Linear(n_step * m, n_class, bias=False)\n",
        "        self.b = nn.Parameter(torch.ones(n_class))\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.C(X) # X : [batch_size, n_step, m]\n",
        "        X = X.view(-1, n_step * m) # [batch_size, n_step * m]\n",
        "        tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden]\n",
        "        output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class]\n",
        "        return output"
      ],
      "metadata": {
        "id": "KhYvFWcgIECf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    n_step = 2 # number of steps, n-1 in paper\n",
        "    n_hidden = 2 # number of hidden size, h in paper\n",
        "    m = 2 # embedding size, m in paper\n",
        "\n",
        "    sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
        "\n",
        "    word_list = \" \".join(sentences).split()\n",
        "    word_list = list(set(word_list))\n",
        "    word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "    number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "    n_class = len(word_dict)  # number of Vocabulary\n",
        "\n",
        "    model = NNLM()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    input_batch, target_batch = make_batch()\n",
        "    input_batch = torch.LongTensor(input_batch)\n",
        "    target_batch = torch.LongTensor(target_batch)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(5000):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_batch)\n",
        "\n",
        "        # output : [batch_size, n_class], target_batch : [batch_size]\n",
        "        loss = criterion(output, target_batch)\n",
        "        if (epoch + 1) % 1000 == 0:\n",
        "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Predict\n",
        "    predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
        "\n",
        "    # Test\n",
        "    print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5wxqTXwH88G",
        "outputId": "fefbc3ff-b4b5-40c3-c8a9-1cd6157066bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1000 cost = 0.049638\n",
            "Epoch: 2000 cost = 0.010600\n",
            "Epoch: 3000 cost = 0.004060\n",
            "Epoch: 4000 cost = 0.001900\n",
            "Epoch: 5000 cost = 0.000976\n",
            "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['dog', 'coffee', 'milk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s6kr-sVDl8U",
        "outputId": "38db08c2-a059-4298-a7c9-c7ed093b33e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "word_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xr5CGjkJ9F-",
        "outputId": "a32c115b-59d9-445b-ce78-13cd048af4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['milk', 'coffee', 'i', 'love', 'like', 'hate', 'dog']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "word_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3AfAqMQKMDc",
        "outputId": "00b0a9a1-66bf-455a-9675-50e1039c90c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'milk': 0, 'coffee': 1, 'i': 2, 'love': 3, 'like': 4, 'hate': 5, 'dog': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "number_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0_2jAorKO-N",
        "outputId": "dec80dc6-bb88-4757-d647-52fb8a2e848f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'milk', 1: 'coffee', 2: 'i', 3: 'love', 4: 'like', 5: 'hate', 6: 'dog'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_class = len(word_dict)\n",
        "n_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S68MwO8tKSfR",
        "outputId": "0780cf76-86b4-49e1-ca3d-3044aac54686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBsGc15iLsds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}